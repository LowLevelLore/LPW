{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from gym import spaces, Env\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# -----------------------\n",
    "# 1. Define the custom environment\n",
    "# -----------------------\n",
    "class CustomEnv(Env):\n",
    "    \"\"\"\n",
    "    A simple custom continuous environment.\n",
    "    \n",
    "    - State: [position, velocity]\n",
    "      * position ∈ [-10, 10]\n",
    "      * velocity ∈ [-10, 10]\n",
    "    - Action: [acceleration]\n",
    "      * acceleration ∈ [-1, 1]\n",
    "    - Dynamics:\n",
    "      position_{t+1} = position_t + velocity_t\n",
    "      velocity_{t+1} = velocity_t + acceleration\n",
    "    - Reward:\n",
    "      * Negative squared error from the target position (5.0)\n",
    "      * Minus a penalty for large acceleration (to encourage smooth control)\n",
    "    - Episode ends when:\n",
    "      * Time step reaches max_steps or position is within 0.1 of the target.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(CustomEnv, self).__init__()\n",
    "        self.target = 5.0\n",
    "        self.max_steps = 200\n",
    "        \n",
    "        # Define action space: one-dimensional continuous with low=-1, high=1.\n",
    "        self.action_space = spaces.Box(low=np.array([-1.0]), high=np.array([1.0]), dtype=np.float32)\n",
    "        \n",
    "        # Define observation space: [position, velocity]\n",
    "        self.observation_space = spaces.Box(low=np.array([-10.0, -10.0]), high=np.array([10.0, 10.0]), dtype=np.float32)\n",
    "        \n",
    "        self.state = None\n",
    "        self.step_count = 0\n",
    "\n",
    "    def reset(self):\n",
    "        # Initialize state: set position and velocity to 0.\n",
    "        self.state = np.array([0.0, 0.0], dtype=np.float32)\n",
    "        self.step_count = 0\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        # Clip action (should be within [-1, 1])\n",
    "        action = np.clip(action, self.action_space.low, self.action_space.high)\n",
    "        \n",
    "        position, velocity = self.state\n",
    "        # Simple dynamics:\n",
    "        position_new = position + velocity\n",
    "        velocity_new = velocity + action[0]  # action is an array of one value\n",
    "        \n",
    "        self.state = np.array([position_new, velocity_new], dtype=np.float32)\n",
    "        self.step_count += 1\n",
    "        \n",
    "        # Reward is negative squared error to the target plus a penalty on the control.\n",
    "        error = self.target - position_new\n",
    "        reward = - (error ** 2) - 0.1 * (action[0] ** 2)\n",
    "        \n",
    "        # Check termination: either reached close to the target or exceeded max steps.\n",
    "        done = bool(abs(error) < 0.1 or self.step_count >= self.max_steps)\n",
    "        return self.state, reward, done, {}\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        # For simplicity, we do not implement rendering here.\n",
    "        pass\n",
    "\n",
    "# -----------------------\n",
    "# 2. Define the Actor-Critic Networks\n",
    "# -----------------------\n",
    "class Actor(nn.Module):\n",
    "    \"\"\"\n",
    "    Actor network that outputs the parameters (mean) of a Gaussian distribution\n",
    "    for the continuous action and learns a log_std (shared or fixed can be used).\n",
    "    \n",
    "    Input: state of shape [batch_size, state_dim]\n",
    "    Output: mean for action of shape [batch_size, action_dim]\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.mean = nn.Linear(hidden_dim, action_dim)\n",
    "        # Learn log_std as a parameter (we can initialize to a small constant)\n",
    "        self.log_std = nn.Parameter(torch.zeros(action_dim))\n",
    "    \n",
    "    def forward(self, state):\n",
    "        # state: [batch_size, state_dim]\n",
    "        x = F.relu(self.fc1(state))      # x: [batch_size, hidden_dim]\n",
    "        x = F.relu(self.fc2(x))          # x: [batch_size, hidden_dim]\n",
    "        mean = self.mean(x)              # mean: [batch_size, action_dim]\n",
    "        # Use softplus to ensure std is positive.\n",
    "        std = torch.exp(self.log_std)    # std: [action_dim], same for every state\n",
    "        return mean, std\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \"\"\"\n",
    "    Critic network that outputs a scalar value given a state.\n",
    "    \n",
    "    Input: state of shape [batch_size, state_dim]\n",
    "    Output: value of shape [batch_size, 1]\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim, hidden_dim=128):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.value = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        # state: [batch_size, state_dim]\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        value = self.value(x)  # shape: [batch_size, 1]\n",
    "        return value\n",
    "\n",
    "# -----------------------\n",
    "# 3. Training Setup for Actor-Critic (A2C)\n",
    "# -----------------------\n",
    "env = CustomEnv()\n",
    "\n",
    "state_dim = env.observation_space.shape[0]     # 2 for our custom env ([position, velocity])\n",
    "action_dim = env.action_space.shape[0]           # 1 for our continuous action\n",
    "\n",
    "actor = Actor(state_dim, action_dim).to(device)\n",
    "critic = Critic(state_dim).to(device)\n",
    "\n",
    "actor_optimizer = optim.Adam(actor.parameters(), lr=1e-3)\n",
    "critic_optimizer = optim.Adam(critic.parameters(), lr=1e-3)\n",
    "\n",
    "# Hyperparameters for training\n",
    "num_episodes = 500\n",
    "gamma = 0.99\n",
    "\n",
    "# To store episode rewards for plotting\n",
    "episode_rewards = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()  # state is a numpy array of shape [2]\n",
    "    state = torch.tensor([state], dtype=torch.float32, device=device)  # shape: [1, 2]\n",
    "    \n",
    "    log_probs = []\n",
    "    values = []\n",
    "    rewards = []\n",
    "    done = False\n",
    "    t = 0\n",
    "    \n",
    "    while not done:\n",
    "        t += 1\n",
    "        \n",
    "        # Get value estimate from Critic\n",
    "        value = critic(state)  # shape: [1, 1]\n",
    "        values.append(value)\n",
    "        \n",
    "        # Get actor output: mean and std for action distribution\n",
    "        mean, std = actor(state)  # mean: [1, 1], std: [1] (broadcastable)\n",
    "        \n",
    "        # Create a normal distribution and sample an action\n",
    "        dist = torch.distributions.Normal(mean, std)\n",
    "        action = dist.sample()  # shape: [1, 1]\n",
    "        log_prob = dist.log_prob(action)\n",
    "        log_probs.append(log_prob)\n",
    "        \n",
    "        # Execute action in environment\n",
    "        # Since env.step() expects a numpy array, convert the action to numpy.\n",
    "        action_np = action.cpu().detach().numpy()[0]\n",
    "        next_state_np, reward, done, _ = env.step(action_np)\n",
    "        \n",
    "        rewards.append(reward)\n",
    "        \n",
    "        # Convert next_state to tensor if not terminal.\n",
    "        if not done:\n",
    "            next_state = torch.tensor([next_state_np], dtype=torch.float32, device=device)\n",
    "        else:\n",
    "            next_state = None\n",
    "        \n",
    "        state = next_state if next_state is not None else torch.zeros((1, state_dim), device=device, dtype=torch.float32)\n",
    "    \n",
    "    # Compute returns (discounted cumulative rewards)\n",
    "    R = 0\n",
    "    returns = []\n",
    "    for r in reversed(rewards):\n",
    "        R = r + gamma * R\n",
    "        returns.insert(0, R)\n",
    "    returns = torch.tensor(returns, dtype=torch.float32, device=device).unsqueeze(1)  # shape: [T, 1]\n",
    "    \n",
    "    # Convert list of values (predicted by critic) to tensor: shape [T, 1]\n",
    "    values = torch.cat(values, dim=0)\n",
    "    log_probs = torch.cat(log_probs, dim=0)\n",
    "    \n",
    "    # Advantage: difference between returns and value estimates\n",
    "    advantages = returns - values.detach()\n",
    "    \n",
    "    # Actor loss: negative of (log_prob * advantage)\n",
    "    actor_loss = - (log_probs * advantages).mean()\n",
    "    # Critic loss: Mean Squared Error between returns and values\n",
    "    critic_loss = F.mse_loss(values, returns)\n",
    "    \n",
    "    # Backpropagation for actor\n",
    "    actor_optimizer.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    actor_optimizer.step()\n",
    "    \n",
    "    # Backpropagation for critic\n",
    "    critic_optimizer.zero_grad()\n",
    "    critic_loss.backward()\n",
    "    critic_optimizer.step()\n",
    "    \n",
    "    episode_rewards.append(sum(rewards))\n",
    "    if (episode+1) % 20 == 0:\n",
    "        print(f\"Episode {episode+1}: Total Reward = {episode_rewards[-1]:.2f}, Episode Length = {t}\")\n",
    "\n",
    "# Plot episode rewards to see progress\n",
    "plt.plot(episode_rewards)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.title(\"Actor-Critic Training on Custom Continuous Environment\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
