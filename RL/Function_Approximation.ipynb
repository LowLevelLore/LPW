{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 finished after 19 timesteps, total reward: 19.0\n",
      "Episode 1 finished after 12 timesteps, total reward: 12.0\n",
      "Episode 2 finished after 13 timesteps, total reward: 13.0\n",
      "Episode 3 finished after 17 timesteps, total reward: 17.0\n",
      "Episode 4 finished after 24 timesteps, total reward: 24.0\n",
      "Episode 5 finished after 24 timesteps, total reward: 24.0\n",
      "Episode 6 finished after 16 timesteps, total reward: 16.0\n",
      "Episode 7 finished after 22 timesteps, total reward: 22.0\n",
      "Episode 8 finished after 14 timesteps, total reward: 14.0\n",
      "Episode 9 finished after 11 timesteps, total reward: 11.0\n",
      "Episode 10 finished after 11 timesteps, total reward: 11.0\n",
      "Episode 11 finished after 9 timesteps, total reward: 9.0\n",
      "Episode 12 finished after 13 timesteps, total reward: 13.0\n",
      "Episode 13 finished after 24 timesteps, total reward: 24.0\n",
      "Episode 14 finished after 13 timesteps, total reward: 13.0\n",
      "Episode 15 finished after 15 timesteps, total reward: 15.0\n",
      "Episode 16 finished after 28 timesteps, total reward: 28.0\n",
      "Episode 17 finished after 9 timesteps, total reward: 9.0\n",
      "Episode 18 finished after 26 timesteps, total reward: 26.0\n",
      "Episode 19 finished after 22 timesteps, total reward: 22.0\n",
      "Episode 20 finished after 11 timesteps, total reward: 11.0\n",
      "Episode 21 finished after 9 timesteps, total reward: 9.0\n",
      "Episode 22 finished after 24 timesteps, total reward: 24.0\n",
      "Episode 23 finished after 27 timesteps, total reward: 27.0\n",
      "Episode 24 finished after 20 timesteps, total reward: 20.0\n",
      "Episode 25 finished after 11 timesteps, total reward: 11.0\n",
      "Episode 26 finished after 11 timesteps, total reward: 11.0\n",
      "Episode 27 finished after 9 timesteps, total reward: 9.0\n",
      "Episode 28 finished after 24 timesteps, total reward: 24.0\n",
      "Episode 29 finished after 22 timesteps, total reward: 22.0\n",
      "Episode 30 finished after 9 timesteps, total reward: 9.0\n",
      "Episode 31 finished after 25 timesteps, total reward: 25.0\n",
      "Episode 32 finished after 29 timesteps, total reward: 29.0\n",
      "Episode 33 finished after 35 timesteps, total reward: 35.0\n",
      "Episode 34 finished after 28 timesteps, total reward: 28.0\n",
      "Episode 35 finished after 19 timesteps, total reward: 19.0\n",
      "Episode 36 finished after 24 timesteps, total reward: 24.0\n",
      "Episode 37 finished after 20 timesteps, total reward: 20.0\n",
      "Episode 38 finished after 26 timesteps, total reward: 26.0\n",
      "Episode 39 finished after 17 timesteps, total reward: 17.0\n",
      "Episode 40 finished after 21 timesteps, total reward: 21.0\n",
      "Episode 41 finished after 39 timesteps, total reward: 39.0\n",
      "Episode 42 finished after 41 timesteps, total reward: 41.0\n",
      "Episode 43 finished after 15 timesteps, total reward: 15.0\n",
      "Episode 44 finished after 41 timesteps, total reward: 41.0\n",
      "Episode 45 finished after 37 timesteps, total reward: 37.0\n",
      "Episode 46 finished after 85 timesteps, total reward: 85.0\n",
      "Episode 47 finished after 241 timesteps, total reward: 241.0\n",
      "Episode 48 finished after 158 timesteps, total reward: 158.0\n",
      "Episode 49 finished after 177 timesteps, total reward: 177.0\n",
      "Episode 50 finished after 105 timesteps, total reward: 105.0\n",
      "Episode 51 finished after 48 timesteps, total reward: 48.0\n",
      "Episode 52 finished after 73 timesteps, total reward: 73.0\n",
      "Episode 53 finished after 69 timesteps, total reward: 69.0\n",
      "Episode 54 finished after 92 timesteps, total reward: 92.0\n",
      "Episode 55 finished after 97 timesteps, total reward: 97.0\n",
      "Episode 56 finished after 87 timesteps, total reward: 87.0\n",
      "Episode 57 finished after 106 timesteps, total reward: 106.0\n",
      "Episode 58 finished after 110 timesteps, total reward: 110.0\n",
      "Episode 59 finished after 99 timesteps, total reward: 99.0\n",
      "Episode 60 finished after 177 timesteps, total reward: 177.0\n",
      "Episode 61 finished after 77 timesteps, total reward: 77.0\n",
      "Episode 62 finished after 84 timesteps, total reward: 84.0\n",
      "Episode 63 finished after 198 timesteps, total reward: 198.0\n",
      "Episode 64 finished after 109 timesteps, total reward: 109.0\n",
      "Episode 65 finished after 124 timesteps, total reward: 124.0\n",
      "Episode 66 finished after 82 timesteps, total reward: 82.0\n",
      "Episode 67 finished after 182 timesteps, total reward: 182.0\n",
      "Episode 68 finished after 88 timesteps, total reward: 88.0\n",
      "Episode 69 finished after 97 timesteps, total reward: 97.0\n",
      "Episode 70 finished after 95 timesteps, total reward: 95.0\n",
      "Episode 71 finished after 120 timesteps, total reward: 120.0\n",
      "Episode 72 finished after 104 timesteps, total reward: 104.0\n",
      "Episode 73 finished after 99 timesteps, total reward: 99.0\n",
      "Episode 74 finished after 125 timesteps, total reward: 125.0\n",
      "Episode 75 finished after 139 timesteps, total reward: 139.0\n",
      "Episode 76 finished after 122 timesteps, total reward: 122.0\n",
      "Episode 77 finished after 155 timesteps, total reward: 155.0\n",
      "Episode 78 finished after 115 timesteps, total reward: 115.0\n",
      "Episode 79 finished after 115 timesteps, total reward: 115.0\n",
      "Episode 80 finished after 107 timesteps, total reward: 107.0\n",
      "Episode 81 finished after 110 timesteps, total reward: 110.0\n",
      "Episode 82 finished after 144 timesteps, total reward: 144.0\n",
      "Episode 83 finished after 141 timesteps, total reward: 141.0\n",
      "Episode 84 finished after 235 timesteps, total reward: 235.0\n",
      "Episode 85 finished after 125 timesteps, total reward: 125.0\n",
      "Episode 86 finished after 191 timesteps, total reward: 191.0\n",
      "Episode 87 finished after 161 timesteps, total reward: 161.0\n",
      "Episode 88 finished after 163 timesteps, total reward: 163.0\n",
      "Episode 89 finished after 152 timesteps, total reward: 152.0\n",
      "Episode 90 finished after 279 timesteps, total reward: 279.0\n",
      "Episode 91 finished after 150 timesteps, total reward: 150.0\n",
      "Episode 92 finished after 213 timesteps, total reward: 213.0\n",
      "Episode 93 finished after 155 timesteps, total reward: 155.0\n",
      "Episode 94 finished after 155 timesteps, total reward: 155.0\n",
      "Episode 95 finished after 157 timesteps, total reward: 157.0\n",
      "Episode 96 finished after 199 timesteps, total reward: 199.0\n",
      "Episode 97 finished after 189 timesteps, total reward: 189.0\n",
      "Episode 98 finished after 156 timesteps, total reward: 156.0\n",
      "Episode 99 finished after 190 timesteps, total reward: 190.0\n",
      "Episode 100 finished after 182 timesteps, total reward: 182.0\n",
      "Episode 101 finished after 196 timesteps, total reward: 196.0\n",
      "Episode 102 finished after 187 timesteps, total reward: 187.0\n",
      "Episode 103 finished after 193 timesteps, total reward: 193.0\n",
      "Episode 104 finished after 156 timesteps, total reward: 156.0\n",
      "Episode 105 finished after 146 timesteps, total reward: 146.0\n",
      "Episode 106 finished after 163 timesteps, total reward: 163.0\n",
      "Episode 107 finished after 188 timesteps, total reward: 188.0\n",
      "Episode 108 finished after 229 timesteps, total reward: 229.0\n",
      "Episode 109 finished after 238 timesteps, total reward: 238.0\n",
      "Episode 110 finished after 263 timesteps, total reward: 263.0\n",
      "Episode 111 finished after 172 timesteps, total reward: 172.0\n",
      "Episode 112 finished after 184 timesteps, total reward: 184.0\n",
      "Episode 113 finished after 189 timesteps, total reward: 189.0\n",
      "Episode 114 finished after 175 timesteps, total reward: 175.0\n",
      "Episode 115 finished after 134 timesteps, total reward: 134.0\n",
      "Episode 116 finished after 193 timesteps, total reward: 193.0\n",
      "Episode 117 finished after 243 timesteps, total reward: 243.0\n",
      "Episode 118 finished after 180 timesteps, total reward: 180.0\n",
      "Episode 119 finished after 175 timesteps, total reward: 175.0\n",
      "Episode 120 finished after 141 timesteps, total reward: 141.0\n",
      "Episode 121 finished after 208 timesteps, total reward: 208.0\n",
      "Episode 122 finished after 138 timesteps, total reward: 138.0\n",
      "Episode 123 finished after 128 timesteps, total reward: 128.0\n",
      "Episode 124 finished after 174 timesteps, total reward: 174.0\n",
      "Episode 125 finished after 138 timesteps, total reward: 138.0\n",
      "Episode 126 finished after 142 timesteps, total reward: 142.0\n",
      "Episode 127 finished after 151 timesteps, total reward: 151.0\n",
      "Episode 128 finished after 127 timesteps, total reward: 127.0\n",
      "Episode 129 finished after 149 timesteps, total reward: 149.0\n",
      "Episode 130 finished after 126 timesteps, total reward: 126.0\n",
      "Episode 131 finished after 116 timesteps, total reward: 116.0\n",
      "Episode 132 finished after 141 timesteps, total reward: 141.0\n",
      "Episode 133 finished after 126 timesteps, total reward: 126.0\n",
      "Episode 134 finished after 121 timesteps, total reward: 121.0\n",
      "Episode 135 finished after 116 timesteps, total reward: 116.0\n",
      "Episode 136 finished after 91 timesteps, total reward: 91.0\n",
      "Episode 137 finished after 121 timesteps, total reward: 121.0\n",
      "Episode 138 finished after 130 timesteps, total reward: 130.0\n",
      "Episode 139 finished after 142 timesteps, total reward: 142.0\n",
      "Episode 140 finished after 126 timesteps, total reward: 126.0\n",
      "Episode 141 finished after 110 timesteps, total reward: 110.0\n",
      "Episode 142 finished after 106 timesteps, total reward: 106.0\n",
      "Episode 143 finished after 124 timesteps, total reward: 124.0\n",
      "Episode 144 finished after 112 timesteps, total reward: 112.0\n",
      "Episode 145 finished after 109 timesteps, total reward: 109.0\n",
      "Episode 146 finished after 135 timesteps, total reward: 135.0\n",
      "Episode 147 finished after 107 timesteps, total reward: 107.0\n",
      "Episode 148 finished after 117 timesteps, total reward: 117.0\n",
      "Episode 149 finished after 109 timesteps, total reward: 109.0\n",
      "Episode 150 finished after 125 timesteps, total reward: 125.0\n",
      "Episode 151 finished after 21 timesteps, total reward: 21.0\n",
      "Episode 152 finished after 134 timesteps, total reward: 134.0\n",
      "Episode 153 finished after 19 timesteps, total reward: 19.0\n",
      "Episode 154 finished after 105 timesteps, total reward: 105.0\n",
      "Episode 155 finished after 30 timesteps, total reward: 30.0\n",
      "Episode 156 finished after 51 timesteps, total reward: 51.0\n",
      "Episode 157 finished after 119 timesteps, total reward: 119.0\n",
      "Episode 158 finished after 108 timesteps, total reward: 108.0\n",
      "Episode 159 finished after 16 timesteps, total reward: 16.0\n",
      "Episode 160 finished after 24 timesteps, total reward: 24.0\n",
      "Episode 161 finished after 17 timesteps, total reward: 17.0\n",
      "Episode 162 finished after 109 timesteps, total reward: 109.0\n",
      "Episode 163 finished after 16 timesteps, total reward: 16.0\n",
      "Episode 164 finished after 131 timesteps, total reward: 131.0\n",
      "Episode 165 finished after 34 timesteps, total reward: 34.0\n",
      "Episode 166 finished after 43 timesteps, total reward: 43.0\n",
      "Episode 167 finished after 122 timesteps, total reward: 122.0\n",
      "Episode 168 finished after 26 timesteps, total reward: 26.0\n",
      "Episode 169 finished after 24 timesteps, total reward: 24.0\n",
      "Episode 170 finished after 201 timesteps, total reward: 201.0\n",
      "Episode 171 finished after 13 timesteps, total reward: 13.0\n",
      "Episode 172 finished after 67 timesteps, total reward: 67.0\n",
      "Episode 173 finished after 93 timesteps, total reward: 93.0\n",
      "Episode 174 finished after 144 timesteps, total reward: 144.0\n",
      "Episode 175 finished after 121 timesteps, total reward: 121.0\n",
      "Episode 176 finished after 133 timesteps, total reward: 133.0\n",
      "Episode 177 finished after 88 timesteps, total reward: 88.0\n",
      "Episode 178 finished after 154 timesteps, total reward: 154.0\n",
      "Episode 179 finished after 135 timesteps, total reward: 135.0\n",
      "Episode 180 finished after 119 timesteps, total reward: 119.0\n",
      "Episode 181 finished after 100 timesteps, total reward: 100.0\n",
      "Episode 182 finished after 157 timesteps, total reward: 157.0\n",
      "Episode 183 finished after 91 timesteps, total reward: 91.0\n",
      "Episode 184 finished after 137 timesteps, total reward: 137.0\n",
      "Episode 185 finished after 133 timesteps, total reward: 133.0\n",
      "Episode 186 finished after 125 timesteps, total reward: 125.0\n",
      "Episode 187 finished after 82 timesteps, total reward: 82.0\n",
      "Episode 188 finished after 135 timesteps, total reward: 135.0\n",
      "Episode 189 finished after 138 timesteps, total reward: 138.0\n",
      "Episode 190 finished after 79 timesteps, total reward: 79.0\n",
      "Episode 191 finished after 141 timesteps, total reward: 141.0\n",
      "Episode 192 finished after 127 timesteps, total reward: 127.0\n",
      "Episode 193 finished after 124 timesteps, total reward: 124.0\n",
      "Episode 194 finished after 131 timesteps, total reward: 131.0\n",
      "Episode 195 finished after 127 timesteps, total reward: 127.0\n",
      "Episode 196 finished after 125 timesteps, total reward: 125.0\n",
      "Episode 197 finished after 128 timesteps, total reward: 128.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "np.bool8 = np.bool\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# A Transition holds (state, action, next_state, reward)\n",
    "# Each element:\n",
    "#   - state: tensor with shape [1, n_observations] (e.g., [1, 4] for CartPole)\n",
    "#   - action: tensor with shape [1, 1] (e.g., [[0]] or [[1]])\n",
    "#   - next_state: tensor with shape [1, n_observations] if not terminal, or None\n",
    "#   - reward: tensor with shape [1] (e.g., [1.0])\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition.\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_observations, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.out = nn.Linear(128, n_actions)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x has shape [batch_size, n_observations]; for a single state, batch_size = 1\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.out(x)  # output shape [batch_size, n_actions]\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9        # Starting value for epsilon in epsilon-greedy policy\n",
    "EPS_END = 0.05         # Minimum epsilon\n",
    "EPS_DECAY = 200        # Controls decay rate of epsilon\n",
    "TARGET_UPDATE = 10     # How often to update the target network (in episodes)\n",
    "LR = 1e-3              # Learning rate for optimizer\n",
    "NUM_EPISODES = 200     # Total number of episodes to train\n",
    "MEMORY_CAPACITY = 10000\n",
    "\n",
    "\n",
    "n_observations = env.observation_space.shape[0]  \n",
    "n_actions = env.action_space.n                   \n",
    "\n",
    "policy_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()  \n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=LR)\n",
    "memory = ReplayMemory(MEMORY_CAPACITY)\n",
    "\n",
    "steps_done = 0  \n",
    "\n",
    "def select_action(state):\n",
    "    \"\"\"\n",
    "    Select an action for the given state using epsilon-greedy policy.\n",
    "    - state: tensor of shape [1, n_observations]\n",
    "    Returns:\n",
    "      action: tensor of shape [1, 1]\n",
    "    \"\"\"\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1.0 * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)\n",
    "\n",
    "def compute_loss(batch):\n",
    "    \"\"\"\n",
    "    Compute the Mean Squared Error (MSE) loss for a batch of transitions.\n",
    "    Each element of the batch is:\n",
    "      - state: [1, n_observations] (e.g., [1, 4])\n",
    "      - action: [1, 1] (e.g., [[0]] or [[1]])\n",
    "      - next_state: [1, n_observations] or None (if terminal)\n",
    "      - reward: [1]\n",
    "    This function loops over each transition, uses an if/else to check if next_state is terminal,\n",
    "    and calculates the Q-value target accordingly.\n",
    "    \"\"\"\n",
    "    losses = []  \n",
    "    for state, action, next_state, reward in zip(batch.state, batch.action, batch.next_state, batch.reward):\n",
    "        q_pred = policy_net(state).gather(1, action)\n",
    "        if next_state is None:\n",
    "            q_target = reward \n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q_next_max = target_net(next_state).max(1)[0]\n",
    "            q_target = reward + GAMMA * q_next_max\n",
    "        loss_i = (q_pred.squeeze() - q_target.squeeze()) ** 2\n",
    "        losses.append(loss_i)\n",
    "    loss = sum(losses) / len(losses)\n",
    "    return loss\n",
    "\n",
    "def optimize_model():\n",
    "    \"\"\"Sample a batch of transitions from memory and perform one optimization step.\"\"\"\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    loss = compute_loss(batch)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        if param.grad is not None:\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "for i_episode in range(NUM_EPISODES):\n",
    "    state, _ = env.reset()\n",
    "    state = torch.tensor([state], device=device, dtype=torch.float32)\n",
    "    total_reward = 0\n",
    "    for t in count():\n",
    "        action = select_action(state) \n",
    "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        total_reward += reward\n",
    "        reward_tensor = torch.tensor([reward], device=device, dtype=torch.float32)\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            next_state = None  \n",
    "        else:\n",
    "            next_state = torch.tensor([observation], device=device, dtype=torch.float32)\n",
    "        memory.push(state, action, next_state, reward_tensor)\n",
    "        if next_state is not None:\n",
    "            state = next_state\n",
    "        else:\n",
    "            state = torch.zeros((1, n_observations), device=device, dtype=torch.float32)\n",
    "        optimize_model()\n",
    "        if terminated or truncated:\n",
    "            episode_durations.append(t + 1)\n",
    "            print(f\"Episode {i_episode} finished after {t+1} timesteps, total reward: {total_reward}\")\n",
    "            break\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "print(\"Training complete\")\n",
    "env.close()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(episode_durations)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Episode Length\")\n",
    "plt.title(\"DQN on CartPole-v1: Episode Lengths Over Time\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
